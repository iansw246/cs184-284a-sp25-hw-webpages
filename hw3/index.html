<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: Arhaan Aggarwal & Ian Wong</div>

		<br>

		Link to webpage:<a href="https://iansw246.github.io/cs184-284a-sp25-hw-webpages/hw3/index.html">https://iansw246.github.io/cs184-284a-sp25-hw-webpages/hw3/index.html</a>
		Link to GitHub repository:<a href="https://github.com/cal-cs184-student/sp25-hw3-hw3.git">https://github.com/cal-cs184-student/sp25-hw3-hw3.git</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<h3>Ray Generation</h3>
<p>Ray generation is the first step in the rendering pipeline, where we compute rays from the camera through each pixel of the image plane. In our implementation, we convert normalized image coordinates into camera space, apply the field of view transformations, and finally transform the ray direction into world space using the camera-to-world transformation matrix.</p>

<ul>
  <li><b>Convert image-space coordinates to camera-space:</b> We use the horizontal (`hFov`) and vertical (`vFov`) field of view to map normalized image coordinates onto the canonical sensor plane.</li>
  <li><b>Compute the ray direction in world space:</b> After computing the ray direction in camera space, we apply the camera-to-world transformation (`c2w`) to transform it into world coordinates.</li>
  <li><b>Normalize the ray direction:</b> Ensuring the ray is unit-length helps with numerical stability in intersection calculations.</li>
  <li><b>Construct the ray:</b> The ray is initialized with the camera's position (`pos`), direction, and clipping planes (`nClip`, `fClip`).</li>
</ul>

<h3>Primitive Intersection</h3>
<p>Once rays are generated, they need to be tested against scene primitives (triangles and spheres) to determine where they intersect the geometry. We implemented efficient algorithms for both triangle and sphere intersections.</p>

<h3>Triangle Intersection Algorithm</h3>
<p>For triangle intersections, we used the Moller-Trumbore algorithm, which efficiently computes ray-triangle intersections without requiring explicit plane equations.</p>

<ul>
  <li><b>Step 1: Compute Intersection Information</b> - We call `moller_trumbore(this, r)`, which computes barycentric coordinates (`u, v`) and the intersection distance (`t`).</li>
  <li><b>Step 2: Validate the Intersection</b> - We check whether the intersection is within the valid ray interval (`r.min_t <= t <= r.max_t`).</li>
  <li><b>Step 3: Update Closest Intersection</b> - If the intersection is valid, we update `r.max_t` to ensure only the closest intersection is stored.</li>
  <li><b>Step 4: Compute the Interpolated Normal</b> - We use barycentric coordinates to interpolate the normal at the hit point for smooth shading.</li>
  <li><b>Step 5: Store Intersection Data</b> - We store the normal, `bsdf`, and intersection `t` value in the `Intersection` struct.</li>
</ul>

<h3>Sphere Intersection Algorithm</h3>
<p>We also implemented ray-sphere intersection using the quadratic formula approach:</p>

<ul>
  <li><b>Step 1: Solve Quadratic Equation</b> - We compute `t1` and `t2` by solving the quadratic equation using the ray direction, origin, and sphere radius.</li>
  <li><b>Step 2: Validate Intersection</b> - We check whether `t1` or `t2` is within the valid range.</li>
  <li><b>Step 3: Compute Surface Normal</b> - The normal at the intersection point is computed as `normalize(intersect_point - sphere_center)`.</li>
  <li><b>Step 4: Store Intersection Data</b> - We update the `Intersection` struct with normal, `bsdf`, and intersection `t`.</li>
</ul>

<h3>Rendered Images with Normal Shading</h3>
<p>Below are images of small `.dae` files rendered with normal shading, showing how our ray generation and intersection algorithms produce correct geometry representations.</p>

<div style="display: flex; flex-direction: column; align-items: center;">
  <table style="width: 100%; text-align: center; border-collapse: collapse;">
    <tr>
      <td style="text-align: center;">
        <img src="images/CBempty.png" width="400px"/>
        <figcaption>CBempty.</figcaption>
      </td>
      <td style="text-align: center;">
        <img src="images/CBspheres.png" width="400px"/>
        <figcaption>CBSpheres.</figcaption>
      </td>
    </tr>
    <tr>
      <td style="text-align: center;">
        <img src="images/cube.png" width="400px"/>
        <figcaption>Cube.</figcaption>
	  </td>
    </tr>
  </table>
</div>

<p>These images show that our ray generation and intersection implementations correctly produce accurate geometric representations in the rendered scene.</p>




		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<h3>BVH Construction Algorithm</h3>
<p>To accelerate ray-scene intersection tests, we implemented a Bounding Volume Hierarchy (BVH) using a recursive top-down construction approach. The BVH is built by partitioning the primitives into two child nodes until the number of primitives in a node is at most a predefined <code>max_leaf_size</code>. The key steps are as follows:</p>

<ul>
  <li><b>Compute Bounding Box:</b> For each node, we calculate a bounding box that encompasses all primitives assigned to that node.</li>
  <li><b>Base Case - Create a Leaf Node:</b> If the number of primitives is <= <code>max_leaf_size</code>, we create a leaf node storing these primitives.</li>
  <li><b>Choosing a Splitting Axis:</b> We determine the axis with the largest extent (x, y, or z) and partition primitives along this axis.</li>
  <li><b>Partitioning:</b> We use <code>std::nth_element()</code> to sort primitives by their centroids along the chosen axis and split at the median.</li>
  <li><b>Recursive Construction:</b> The primitive list is split into two subsets, and we recursively build the left and right child nodes.</li>
</ul>

<h3>Heuristic for Choosing the Splitting Point</h3>
<p>We chose the <b>median split heuristic</b> for partitioning primitives. By splitting along the axis with the largest extent, we aim to achieve a balanced BVH with minimal overlap between bounding volumes. This heuristic provides a good trade-off between computational efficiency and spatial coherence, reducing the number of intersection tests required during rendering.</p>

<h3>Rendered Images with BVH Acceleration</h3>
<p>Below are images of complex <code>.dae</code> models that could only be rendered efficiently using BVH acceleration.</p>

<div style="display: flex; flex-direction: column; align-items: center;">
  <table style="width: 100%; text-align: center; border-collapse: collapse;">
    <tr>
      <td style="text-align: center;">
        <img src="images/cow_bvh.png" width="400px"/>
        <figcaption>Rendered cow model using BVH.</figcaption>
      </td>
      <td style="text-align: center;">
        <img src="images/maxplank.png" width="400px"/>
        <figcaption>Rendered Max Planck model using BVH.</figcaption>
      </td>
    </tr>
    <tr>
      <td style="text-align: center;">
        <img src="images/CBlucy.png" width="400px"/>
        <figcaption>Rendered CBlucy model using BVH.</figcaption>
      </td>
      <td style="text-align: center;">
        <img src="images/dragon.png" width="400px"/>
        <figcaption>Rendered Dragon model using BVH.</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3>Performance Comparison</h3>
<p>We tested our BVH implementation on moderately complex geometries and compared rendering times with and without BVH acceleration.</p>
<p>Using 8 threads, without BVH, rendering <code>cow.dae</code> took 112s. With BVH, rendering took 0.0592s.</p>

<h3>Analysis</h3>
<p>The results clearly show that BVH acceleration significantly improves rendering times, achieving upwards of 200x speedups for moderately complex models. Without BVH, the path tracer must check for intersections against every primitive, leading to O(n) complexity. With BVH, our logarithmic O(log n) traversal drastically reduces the number of intersection tests, making rendering feasible even for large scenes.</p>

<p>While our median split heuristic performed well, using a more advanced Surface Area Heuristic (SAH) could further optimize partitioning and traversal. Nonetheless, our implementation demonstrates the crucial role of BVH in accelerating ray tracing, making complex scene rendering significantly more efficient.</p>

		<h2>Part 3: Direct Illumination</h2>
		
		<h3>Implementation</h3>
		<p>
			The general algorithm to make a Monte Carlo estimate
			of the direct lighting at a point \( p \)
			and along a ray in direction \( \omega_r \).
			A sample is taken the light from an incoming direction \( \omega_j \),
			with sample sample having a probability of \( p(\omega_j) \),
			equivalent to sampling a ray that starts from the given point.
			If the sampled ray intersects a light,
			then the emitted lighting from the light \( L_i(p, \omega_j) \)
			is used to calculate the incoming light along the original ray
			and added to the estimate.
		</p>
		<p>
			The reflectance from the point on the original object is calculated from
			the object material's BSDF: \( f_r(\omega_i \to \omega_j) \).
			For diffuse objects, the BSDF is constant: \( \frac{\rho}{pi} \)
			where \( \rho \) is the material's reflectance spectrum.
		</p>
		<p>
			Each sample of the incoming light is calculated using the reflection equation, where each sample is
			\[
				\frac{f_r(p, \omega_j \to \omega_r) L_i(p, \omega_j) \cos \theta_j}{p(\omega_j)}
			\]
		</p>
		<p>
			The numerator is the lighting equation.
			The denominator is the normalizing constant for the Monte Carlo estimate.
		</p>
		<p>
			A fixed number of samples is taken and the samples are averaged
			to produce the final estimate.
		</p>
		<h4>Uniform hemisphere sampling</h4>
		<p>
			The difference between uniform hemisphere sampling and light importance sampling
			is in how the samples for the incoming light rays are taken.

			Uniform hemisphere sampling samples directions uniformly from the
			hemisphere above a point, with each sample having probability of
			\( \frac{1}{2\pi} \)
		</p>
		<h4>Light importance sampling</h4>
		<p>
			Light importance sampling only samples rays in the direction
			of a light, only considering samples that have a chance of contribute
			to lighting.
			For each light in the scene, samples  from target point to the light are taken.
		</p>
		<p>
			Point lights have only have one sample taken because there is only one
			path for a ray from the light to the point.

			Area lights have a fixed number of samples taken.
		</p>

		<table>
			<tr>
				<td>
					<figure>
						<img src="images/part3/CBspheres_H64_32.png">
						<figcaption>Uniform hemisphere sampling</figcaption>
					</figure>
				</td>
				<td>
					<figure>
						<img src="images/part3/CBspheres_64_32.png">
						<figcaption>Light importance sampling</figcaption>
					</figure>
				</td>
			</tr>
			<tr>
				<td>
					<figure>
						<img src="images/part3/CBbunny_H_64_32.png">
						<figcaption>Uniform hemisphere sampling</figcaption>
					</figure>
				</td>
				<td>
					<figure>
						<img src="images/part3/CBbunny_64_32.png">
						<figcaption>Light importance sampling</figcaption>
					</figure>
				</td>
			</tr>
		</table>

		
		
		<h3>Comparison of number of rays per light using light sampling.</h3>
		<p>All images generated with 1 sample per pixel</p>
		<p>
			Notice the great reduction in noise across the entire image,
			especially in the soft shadow of the bunny under it.
		</p>
		
		<figure>
			<img src="images/part3/bunny_1_1.png">
			<figcaption>1 light ray per pixel</figcaption>
		</figure>

		<figure>
			<img src="images/part3/bunny_1_4.png">
			<figcaption>4 light rays per pixel</figcaption>
		</figure>

		<figure>
			<img src="images/part3/bunny_1_16.png">
			<figcaption>16 light rays per pixel</figcaption>
		</figure>

		<figure>
			<img src="images/part3/bunny_1_64.png">
			<figcaption>64 light rays per pixel</figcaption>
		</figure>

		<h3>Comparision between uniform hemisphere sampling and lighting sampling</h3>

		<p>
			Lighting sampling generally has higher efficiency than uniform hemisphere
			sampling, meaning less noise for the same number of light rays or rays per pixel.
			In the comparison images above, this difference is most noticeable
			in the walls and floor.

			This is highly desirable to reduce the amount of compute required
			for the same quality image.
		</p>
		<p>
			This comes from the fact that lighting sampling only checks rays that
			are in the direction of a light rather than in all directions.
			The sampling distribution with lighting sampling
			is closer to the true distribution compared to a uniform distribution,
			so the variance is reduced.
		</p>

		<h2>Part 4: Global Illumination</h2>
		<h3>Implementation</h3>
		<p>
			For each initial ray, first the zero bounce lighting is computed.
			Then the global illumination is calculated recursively by calculating
			the radiance from light that has at least one bounce.
		</p>
		<p>
			At least one bounce radiance is calculated depending on the ray people depth.
			<ul>
				<li>
					Ray depth less than one: Return black: ray depth max depth exeeded.
				</li>
				<li>
					Ray depth equal to one: Return one bounce radiance (direct lighting):
					reached max ray depth. Base case for recursion.
				</li>
			</ul>
			Otherwise, we enter the main body.
		</p>

		<p>
			If <code>isAccumBounces</code> is true, we add the one_bounce_radiance
			at the current point to the result radiance.
		</p>
		<p>
			We also perform Russian Roulette ray termination by returning the
			radiance early with some probability, which is set to 0.3 in
			this implementation.
		</p>
		<p>
			Otherwise, we then sample a new light ray from the current point.
			If that new ray doesn't intersect the scene, the function returns
			the radiance. Otherwise, it recurses with the new ray and intersection point
			to compute the radiance from the new ray.
			That incoming radiance is used to calulate the outgoing radiance
			at the current point according to the lighting equation,
			similar to one bounce radiance.
			Finally, the outgoing radiance is returned.
		</p>

		<h3>Global illumination examples</h3>
		<figure>
			<img src="images/part4/bunny_1024_16.png">
		</figure>
		<figure>
			<img src="images/part4/spheres_1024_8.png">
		</figure>

		<h3>Direct and indirect illumination comparision</h3>
		<figure>
			<img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m1.png">
			<figcaption>Only direct lighting</figcaption>
		</figure>
		<figure>
			<img src="images/part4/indirect_only/bunny_indirect_only_1024_16.png">
			<figcaption>Only indirect lighting</figcaption>
		</figure>

		Notice the colors on the bunny, wall, and ceiling from indirect lighting,
		while all lighting in the direct lighting image is white.

		<h3>Lighting from each bounce<h3>
		<p>
			These images are rendered without accumulating the light from previous bounces,
			showing the contribution of each bounce of light.
		</p>
		<table>
			<tr>
				<th>Ray depth</th>
				<th>Image</th>
			<tr>
				<td>0</td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m0.png"></td>
			</tr>
			<tr>
				<td>1</td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m1.png"></td>
			</tr>
			<tr>
				<td>2</td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m2.png"></td>
			</tr>
			<tr>
				<td>3</td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m3.png"></td>
			</tr>
			<tr>
				<td>4</td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m4.png"></td>
			</tr>
			<tr>
				<td>5</td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m5.png"></td>
			</tr>
		</table>
		<p>
			The second bounce has reflected light from the wall on the bunny and the floor,
			adding the red and blue shades to the bunny and floor in the complete image.
			It also adds reflected light from the floor to the underside of the bunny,
			preventing the bottom from being completely black.
		</p>
		<p>
			The third bounce adds further red and blue shading to the bunny and floor.
			Furthermore, the ceiling now has indirect lighting from the colored walls,
			visible in the red and blue colors.
		</p>

		<h3>Accumulated vs Unaccumulated bounces</h3>
		<table>
			<tr>
				<th>Max ray depth</th>
				<th>Accumulated bounces</th>
				<th>Unaccumulated bounces</th>
			</tr>
			<tr>
				<td>0</td>
				<td><img src="images/part4/bunny_accum/bunny_1024_accum_m0.png"></td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m0.png"></td>
			</tr>
			<tr>
				<td>1</td>
				<td><img src="images/part4/bunny_accum/bunny_1024_accum_m1.png"></td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m1.png"></td>
			</tr>
			<tr>
				<td>2</td>
				<td><img src="images/part4/bunny_accum/bunny_1024_accum_m2.png"></td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m2.png"></td>
			</tr>
			<tr>
				<td>3</td>
				<td><img src="images/part4/bunny_accum/bunny_1024_accum_m3.png"></td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m3.png"></td>
			</tr>
			<tr>
				<td>4</td>
				<td><img src="images/part4/bunny_accum/bunny_1024_accum_m4.png"></td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m4.png"></td>
			</tr>
			<tr>
				<td>5</td>
				<td><img src="images/part4/bunny_accum/bunny_1024_accum_m5.png"></td>
				<td><img src="images/part4/bunny_no_accum/bunny_1024_unaccum_m5.png"></td>
			</tr>
		</table>
		

		<h3>Russian Roulette</h3>
		<table>
			<tr>
				<th>Ray depth</th>
				<th>Image</th>
			<tr>
				<td>0</td>
				<td><img src="images/part4/bunny_ray_depth_compare/bunny_1024_m_0.png"></td>
			</tr>
			<tr>
				<td>1</td>
				<td><img src="images/part4/bunny_ray_depth_compare/bunny_1024_m_1.png"></td>
			</tr>
			<tr>
				<td>2</td>
				<td><img src="images/part4/bunny_ray_depth_compare/bunny_1024_m_2.png"></td>
			</tr>
			<tr>
				<td>3</td>
				<td><img src="images/part4/bunny_ray_depth_compare/bunny_1024_m_3.png"></td>
			</tr>
			<tr>
				<td>4</td>
				<td><img src="images/part4/bunny_ray_depth_compare/bunny_1024_m_4.png"></td>
			</tr>
			<tr>
				<td>100</td>
				<td><img src="images/part4/bunny_ray_depth_compare/bunny_1024_m_100.png"></td>
			</tr>
		</table>

		<h3>Samples per pixel comparison</h3>
		<p>Rendered using 4 samples per light</p>
		<table>
			<tr>
				<th>Samples per pixel</th>
				<th>Image</th>
			<tr>
				<td>1</td>
				<td><img src="images/part4/sample_per_pixel_compare/bunny_1_4.png"></td>
			</tr>
			<tr>
				<td>2</td>
				<td><img src="images/part4/sample_per_pixel_compare/bunny_2_4.png"></td>
			</tr>
			<tr>
				<td>4</td>
				<td><img src="images/part4/sample_per_pixel_compare/bunny_4_4.png"></td>
			</tr>
			<tr>
				<td>8</td>
				<td><img src="images/part4/sample_per_pixel_compare/bunny_8_4.png"></td>
			</tr>
			<tr>
				<td>16</td>
				<td><img src="images/part4/sample_per_pixel_compare/bunny_16_4.png"></td>
			</tr>
			<tr>
				<td>64</td>
				<td><img src="images/part4/sample_per_pixel_compare/bunny_64_4.png"></td>
			</tr>
			<tr>
				<td>1024</td>
				<td><img src="images/part4/sample_per_pixel_compare/bunny_1024_4.png"></td>
			</tr>
		</table>


		<h2>Part 5: Adaptive Sampling</h2>
		Adaptive sampling aims to improve the efficiency of rendering noise-free images
		by using more samples for difficult, noisy pixels.

		<p>
			This is accomplished by tracking the mean and variance of samples for each pixel
			and stopping sampling early if the margin of error is small enough.
		</p>
		<p>
			For each pixel, the running sum of the illuminance \( s_1 \) of each sample
			and running sum of square illuminance \(s_2 \) is tracked.
			These values are used to calculate the mean and variance as
			\( \mu = \frac{s_1}{n} \) and \( \sigma^2 = \frac{1}{n - 1} \left(s_2 - \frac{s_1^2}{n} \right) \),
			where \(n \) is the number of samples so far.
		</p>
		<p>
			\( I = 1.96 \left( \frac{\sigma}{\sqrt{n}} \right) \) measures the pixel's convergence.
			If \(I \le k \cdot \mu \), for \(k \) representing the max tolerance,
			the pixel is considered converged and rendering moves on to the next pixel.
			\(k = 0.05 \) by default.
		</p>

		<h3>Sampling rate demonstrations</h3>
		<figure>
			<img src="images/part5/bunny_2048_1.png">
			<img src="images/part5/bunny_2048_1_rate.png">
		
			<figcaption>
				Rendered image and the sampling rate per pixel.
				Red represents high sampling rates, blue for low.
				Rendered with 1 sample per light, max ray depth of 5,
				at most 2048 samples per pixel.
			</figcaption>
		</figure>

		<figure>
			<img src="images/part5/spheres_2048_1.png">
			<img src="images/part5/spheres_2048_1_rate.png">
		
			<figcaption>
				Rendered image and the sampling rate per pixel.
				Red represents high sampling rates, blue for low.
				Rendered with 1 sample per light, max ray depth of 5,
				at most 2048 samples per pixel.
			</figcaption>
		</figure>

		<!-- <h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. -->
		</div>
	</body>
</html>