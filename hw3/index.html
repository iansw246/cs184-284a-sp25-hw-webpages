<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
				font-family: 'Inter', sans-serif;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>CS184/284A Spring 2025 Homework 3 Write-Up</h1>
		<div style="text-align: center;">Names: </div>

		<br>

		Link to webpage: (TODO) <a href="https://cs184.eecs.berkeley.edu/sp25">cs184.eecs.berkeley.edu/sp25</a>
		Link to GitHub repository: (TODO) <a href="https://cs184.eecs.berkeley.edu/sp25">cs184.eecs.berkeley.edu/sp25</a>
		
		<figure>
			<img src="cornell.png" alt="Cornell Boxes with Bunnies" style="width:70%"/>
			<figcaption>You can add images with captions!</figcaption>
		</figure>

		<!--
		We've already added one heading per part, to make your write-up as navigable when grading. Please fit your write-up within these sections!
		-->

		<h2>Overview</h2>
		Give a high-level overview of what you implemented in this homework. Think about what you've built as a whole. Share your thoughts on what interesting things you've learned from completing the homework.

		<h2>Part 1: Ray Generation and Scene Intersection</h2>
		<h3>Ray Generation</h3>
<p>Ray generation is the first step in the rendering pipeline, where we compute rays from the camera through each pixel of the image plane. In our implementation, we convert normalized image coordinates into camera space, apply the field of view transformations, and finally transform the ray direction into world space using the camera-to-world transformation matrix.</p>

<ul>
  <li><b>Convert image-space coordinates to camera-space:</b> We use the horizontal (`hFov`) and vertical (`vFov`) field of view to map normalized image coordinates onto the canonical sensor plane.</li>
  <li><b>Compute the ray direction in world space:</b> After computing the ray direction in camera space, we apply the camera-to-world transformation (`c2w`) to transform it into world coordinates.</li>
  <li><b>Normalize the ray direction:</b> Ensuring the ray is unit-length helps with numerical stability in intersection calculations.</li>
  <li><b>Construct the ray:</b> The ray is initialized with the camera's position (`pos`), direction, and clipping planes (`nClip`, `fClip`).</li>
</ul>

<h3>Primitive Intersection</h3>
<p>Once rays are generated, they need to be tested against scene primitives (triangles and spheres) to determine where they intersect the geometry. We implemented efficient algorithms for both triangle and sphere intersections.</p>

<h3>Triangle Intersection Algorithm</h3>
<p>For triangle intersections, we used the Moller-Trumbore algorithm, which efficiently computes ray-triangle intersections without requiring explicit plane equations.</p>

<ul>
  <li><b>Step 1: Compute Intersection Information</b> - We call `moller_trumbore(this, r)`, which computes barycentric coordinates (`u, v`) and the intersection distance (`t`).</li>
  <li><b>Step 2: Validate the Intersection</b> - We check whether the intersection is within the valid ray interval (`r.min_t <= t <= r.max_t`).</li>
  <li><b>Step 3: Update Closest Intersection</b> - If the intersection is valid, we update `r.max_t` to ensure only the closest intersection is stored.</li>
  <li><b>Step 4: Compute the Interpolated Normal</b> - We use barycentric coordinates to interpolate the normal at the hit point for smooth shading.</li>
  <li><b>Step 5: Store Intersection Data</b> - We store the normal, `bsdf`, and intersection `t` value in the `Intersection` struct.</li>
</ul>

<h3>Sphere Intersection Algorithm</h3>
<p>We also implemented ray-sphere intersection using the quadratic formula approach:</p>

<ul>
  <li><b>Step 1: Solve Quadratic Equation</b> - We compute `t1` and `t2` by solving the quadratic equation using the ray direction, origin, and sphere radius.</li>
  <li><b>Step 2: Validate Intersection</b> - We check whether `t1` or `t2` is within the valid range.</li>
  <li><b>Step 3: Compute Surface Normal</b> - The normal at the intersection point is computed as `normalize(intersect_point - sphere_center)`.</li>
  <li><b>Step 4: Store Intersection Data</b> - We update the `Intersection` struct with normal, `bsdf`, and intersection `t`.</li>
</ul>

<h3>Rendered Images with Normal Shading</h3>
<p>Below are images of small `.dae` files rendered with normal shading, showing how our ray generation and intersection algorithms produce correct geometry representations.</p>

<div style="display: flex; flex-direction: column; align-items: center;">
  <table style="width: 100%; text-align: center; border-collapse: collapse;">
    <tr>
      <td style="text-align: center;">
        <img src="images/CBempty.png" width="400px"/>
        <figcaption>CBempty.</figcaption>
      </td>
      <td style="text-align: center;">
        <img src="images/CBspheres.png" width="400px"/>
        <figcaption>CBSpheres.</figcaption>
      </td>
    </tr>
    <tr>
      <td style="text-align: center;">
        <img src="images/cube.png" width="400px"/>
        <figcaption>Cube.</figcaption>
	  </td>
    </tr>
  </table>
</div>

<p>These images show that our ray generation and intersection implementations correctly produce accurate geometric representations in the rendered scene.</p>




		
		<h2>Part 2: Bounding Volume Hierarchy</h2>
		<h3>BVH Construction Algorithm</h3>
<p>To accelerate ray-scene intersection tests, we implemented a Bounding Volume Hierarchy (BVH) using a recursive top-down construction approach. The BVH is built by partitioning the primitives into two child nodes until the number of primitives in a node is at most a predefined <code>max_leaf_size</code>. The key steps are as follows:</p>

<ul>
  <li><b>Compute Bounding Box:</b> For each node, we calculate a bounding box that encompasses all primitives assigned to that node.</li>
  <li><b>Base Case - Create a Leaf Node:</b> If the number of primitives is <= <code>max_leaf_size</code>, we create a leaf node storing these primitives.</li>
  <li><b>Choosing a Splitting Axis:</b> We determine the axis with the largest extent (x, y, or z) and partition primitives along this axis.</li>
  <li><b>Partitioning:</b> We use <code>std::nth_element()</code> to sort primitives by their centroids along the chosen axis and split at the median.</li>
  <li><b>Recursive Construction:</b> The primitive list is split into two subsets, and we recursively build the left and right child nodes.</li>
</ul>

<h3>Heuristic for Choosing the Splitting Point</h3>
<p>We chose the <b>median split heuristic</b> for partitioning primitives. By splitting along the axis with the largest extent, we aim to achieve a balanced BVH with minimal overlap between bounding volumes. This heuristic provides a good trade-off between computational efficiency and spatial coherence, reducing the number of intersection tests required during rendering.</p>

<h3>Rendered Images with BVH Acceleration</h3>
<p>Below are images of complex <code>.dae</code> models that could only be rendered efficiently using BVH acceleration.</p>

<div style="display: flex; flex-direction: column; align-items: center;">
  <table style="width: 100%; text-align: center; border-collapse: collapse;">
    <tr>
      <td style="text-align: center;">
        <img src="images/cow_bvh.png" width="400px"/>
        <figcaption>Rendered cow model using BVH.</figcaption>
      </td>
      <td style="text-align: center;">
        <img src="images/maxplank.png" width="400px"/>
        <figcaption>Rendered Max Planck model using BVH.</figcaption>
      </td>
    </tr>
    <tr>
      <td style="text-align: center;">
        <img src="images/CBlucy.png" width="400px"/>
        <figcaption>Rendered CBlucy model using BVH.</figcaption>
      </td>
      <td style="text-align: center;">
        <img src="images/dragon.png" width="400px"/>
        <figcaption>Rendered Dragon model using BVH.</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3>Performance Comparison</h3>
<p>We tested our BVH implementation on moderately complex geometries and compared rendering times with and without BVH acceleration.</p>

<h3>Analysis</h3>
<p>The results clearly show that BVH acceleration significantly improves rendering times, achieving 7-9x speedup for moderately complex models. Without BVH, the path tracer must check for intersections against every primitive, leading to O(n) complexity. With BVH, our logarithmic O(log n) traversal drastically reduces the number of intersection tests, making rendering feasible even for large scenes.</p>

<p>While our median split heuristic performed well, using a more advanced Surface Area Heuristic (SAH) could further optimize partitioning and traversal. Nonetheless, our implementation demonstrates the crucial role of BVH in accelerating ray tracing, making complex scene rendering significantly more efficient.</p>

		<h2>Part 3: Direct Illumination</h2>
		
		<h3>Implementation</h3>
		<p>
			The general algorithm to make a Monte Carlo estimate
			of the direct lighting at a point \( p \)
			and along a ray in direction \( \omega_r \).
			A sample is taken the light from an incoming direction \( \omega_j \),
			with sample sample having a probability of \( p(\omega_j) \),
			equivalent to sampling a ray that starts from the given point.
			If the sampled ray intersects a light,
			then the emitted lighting from the light \( L_i(p, \omega_j) \)
			is used to calculate the incoming light along the original ray
			and added to the estimate.
		</p>
		<p>
			The reflectance from the point on the original object is calculated from
			the object material's BSDF: \( f_r(\omega_i \to \omega_j) \).
			For diffuse objects, the BSDF is constant: \( \frac{\rho}{pi} \)
			where \( \rho \) is the material's reflectance spectrum.
		</p>
		<p>
			Each sample of the incoming light is calculated using the reflection equation, where each sample is
			\[
				\frac{f_r(p, \omega_j \to \omega_r) L_i(p, \omega_j) \cos \theta_j}{p(\omega_j)}
			\]
		</p>
		<p>
			The numerator is the lighting equation.
			The denominator is the normalizing constant for the Monte Carlo estimate.
		</p>
		<p>
			A fixed number of samples is taken and the samples are averaged
			to produce the final estimate.
		</p>
		<h4>Uniform hemisphere sampling</h4>
		<p>
			The difference between uniform hemisphere sampling and light importance sampling
			is in how the samples for the incoming light rays are taken.

			Uniform hemisphere sampling samples directions uniformly from the
			hemisphere above a point, with each sample having probability of
			\( \frac{1}{2\pi} \)
		</p>
		<h4>Light importance sampling</h4>
		<p>
			Light importance sampling only samples rays in the direction
			of a light, only considering samples that have a chance of contribute
			to lighting.
			For each light in the scene, samples  from target point to the light are taken.
		</p>
		<p>
			Point lights have only have one sample taken because there is only one
			path for a ray from the light to the point.

			Area lights have a fixed number of samples taken.
		</p>

		<table>
			<tr>
				<td>
					<figure>
						<img src="images/part3/CBspheres_H64_32.png">
						<figcaption>Uniform hemisphere sampling</figcaption>
					</figure>
				</td>
				<td>
					<figure>
						<img src="images/part3/CBspheres_64_32.png">
						<figcaption>Light importance sampling</figcaption>
					</figure>
				</td>
			</tr>
			<tr>
				<td>
					<figure>
						<img src="images/part3/CBbunny_H_64_32.png">
						<figcaption>Uniform hemisphere sampling</figcaption>
					</figure>
				</td>
				<td>
					<figure>
						<img src="images/part3/CBbunny_64_32.png">
						<figcaption>Light importance sampling</figcaption>
					</figure>
				</td>
			</tr>
		</table>

		
		
		<h3>Comparison of number of rays per light using light sampling.</h3>
		<p>All images generated with 1 sample per pixel</p>
		<p>
			Notice the great reduction in noise across the entire image,
			especially in the soft shadow of the bunny under it.
		</p>
		
		<figure>
			<img src="images/part3/bunny_1_1.png">
			<figcaption>1 light ray per pixel</figcaption>
		</figure>

		<figure>
			<img src="images/part3/bunny_1_4.png">
			<figcaption>4 light rays per pixel</figcaption>
		</figure>

		<figure>
			<img src="images/part3/bunny_1_16.png">
			<figcaption>16 light rays per pixel</figcaption>
		</figure>

		<figure>
			<img src="images/part3/bunny_1_64.png">
			<figcaption>64 light rays per pixel</figcaption>
		</figure>

		<h3>Comparision between uniform hemisphere sampling and lighting sampling</h3>

		<p>
			Lighting sampling generally has higher efficiency than uniform hemisphere
			sampling, meaning less noise for the same number of light rays or rays per pixel.
			In the comparison images above, this difference is most noticeable
			in the walls and floor.

			This is highly desirable to reduce the amount of compute required
			for the same quality image.
		</p>
		<p>
			This comes from the fact that lighting sampling only checks rays that
			are in the direction of a light rather than in all directions.
			The sampling distribution with lighting sampling
			is closer to the true distribution compared to a uniform distribution,
			so the variance is reduced.
		</p>

		<h2>Part 4: Global Illumination</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>Part 5: Adaptive Sampling</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

		<h2>(Optional) Part 6: Extra Credit Opportunities</h2>
		Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.
		
		<h2>Additional Notes (please remove)</h2>
		<ul>
			<li>You can also add code if you'd like as so: <code>code code code</code></li>
			<li>If you'd like to add math equations, 
				<ul>
					<li>You can write inline equations like so: \( a^2 + b^2 = c^2 \)</li>
					<li>You can write display equations like so: \[ a^2 + b^2 = c^2 \]</li>
				</ul>
			</li>
		</ul>
		</div>
	</body>
</html>